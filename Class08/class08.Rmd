---
output:
  pdf_document: default
  html_document: default
---
___
title: "Machine learning 1"
author: "Anel Valdez" 
date: 10/21/2021
output: 


First up is clustering methods!
# Kmeans clustering

The function in base R to do Kmeans clustering is called 'kmeans()'

first make up some data wher we know what the answer should be:

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3))
x <- cbind(x=tmp, y=rev(tmp))
x
```

> Q. Can we use kmeans() to cluster this data setting k to 2 and nstart to 20?

```{r}
km <- kmeans(x, centers = 2, nstart=20)
km
```

> Q. How many points in each cluster?

```{r}
km$size
```

> Q. What 'component' of your result object details cluster assignment/ membership?

```{r}
km$cluster
```

> Q. What 'component' of your result object details cluster center?

```{r}
km$centers
```

>Q. Plot x colored by the kmeans cluster assignment and
 add cluster centers as blue points
 
```{r}
plot(x, col= km$cluster)
points(km$centers, col="blue", pch=15, cex=2)
```
 
 
 #hclust

A big limitation with k-means is that we have to tell it K (the number of clsuters we want).


Analyze the same data with hclust()

Demonstarte the use of dist(), hclust(), plot() and cutree() functions to do clustering, 
Generate dedrograms and return clsuter assignment/ membership
vector...

```{r}
hc <- hclust( dist(x) )
hc
```


There is a plot method for hclust result objects. Let's see it.

```{r}
plot(hc)
```

To get our cluster membership vector we have to do a little more work. We have to "cut" the tree where we think it makes sense. For this we use the 'cutree()'function. 

```{r}
cutree(hc, h=6)
```

You can also call 'cutree()' setting k=the number of grps/clusters you want,

```{r}
grps <- cutree(hc, k=2)
```

Make our results plot

```{r}
plot(x, col=grps)
```

# Principal 

> Q1. How many rows and columns are in your new data frame named x? What R functions could you use to answer this questions?

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url)
```

```{r}
nrow(x)
```

```{r}
ncol(x)
```
```{r}
dim(x)
```

```{r}
View(x)
```

# Note how the minus indexing works
```{r}
rownames(x) <- x[,1]
x <- x[,-1]
head(x)
```

```{r}
dim(x)
```

```{r}
x <- read.csv(url, row.names=1)
head(x)
```

```{r}
url <- "https://tinyurl.com/UK-foods"
x <- read.csv(url, row.names = 1)
head(x)
```

> Q2. Which approach to solving the ‘row-names problem’ mentioned above do you prefer and why? Is one approach more robust than another under certain circumstances?

The second approach is better because it doesn't delete rows.


```{r}
barplot(as.matrix(x), beside=T, col=rainbow(nrow(x)))
```

```{r}
barplot(as.matrix(x), col=rainbow(10))
```


```{r}
pairs(x, col=rainbow(10), pch=16)
```
> Q5: Generating all pairwise plots may help somewhat. Can you make sense of the following code and resulting figure? What does it mean if a given point lies on the diagonal for a given plot?

The countries are being plotted against each other. If the point are not on the diagonal it means that the values are dissimilar in terms of drinking.

> Q6. What is the main differences between N. Ireland and the other countries of the UK in terms of this data-set?

The main difference is that it has the most different data compared to the UK.

The main function in base R is prcomp()
This want's the transpose 
```{r}
pca <- prcomp( t(x) )
summary(pca)
```
```{r}
attributes(pca)
```

```{r}
plot(pca$x[,1], pca$x[,2])
```


> Q7. Complete the code below to generate a plot of PC1 vs PC2. The second line adds text labels over the data points

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x))
```


```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2", xlim=c(-270,500))
text(pca$x[,1], pca$x[,2], colnames(x), col= c("orange", "red", "blue", "green"))
```

```{r}
v <- round( pca$sdev^2/sum(pca$sdev^2) * 100 )
v
```

```{r}
z <- summary(pca)
z$importance
```

```{r}
barplot(v, xlab="Principal Component", ylab="Percent Variation")
```


```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,1], las=2 )
```

```{r}
par(mar=c(10, 3, 0.35, 0))
barplot( pca$rotation[,2], las=2 )
```

The two groups that are featured prominantely are fresh potatoes and soft drinks. PC2 tells us about the second axis and its variability.


```{r}
biplot(pca)
```


```{r}
url2 <- "https://tinyurl.com/expression-CSV"
rna.data <- read.csv(url2, row.names=1)
head(rna.data)
```
```{r}
nrow(rna.data)
```
```{r}
ncol(rna.data)
```

10 genes and 100 samples

```{r}
pca <- prcomp(t(rna.data), scale=TRUE)
```

```{r}
plot(pca$x[,1], pca$x[,2], xlab="PC1", ylab="PC2")
```

```{r}
summary(pca)
```


```{r}
plot(pca, main="Quick scree plot")
```
```{r}
pca.var <- pca$sdev^2
```

```{r}
pca.var.per <- round(pca.var/sum(pca.var)*100, 1)
pca.var.per
```

```{r}
barplot(pca.var.per, main="Scree Plot", 
        names.arg = paste0("PC", 1:10),
        xlab="Principal Component", ylab="Percent Variation")
```

```{r}
colvec <- colnames(rna.data)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16,
     xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

text(pca$x[,1], pca$x[,2], labels = colnames(rna.data), pos=c(rep(4,5), rep(2,5)))
```

```{r}
library(ggplot2)

df <- as.data.frame(pca$x)

# Our first basic plot
ggplot(df) + 
  aes(PC1, PC2) + 
  geom_point()

```

 Add a 'wt' and 'ko' "condition" column
```{r}
df$samples <- colnames(rna.data) 
df$condition <- substr(colnames(rna.data),1,2)

p <- ggplot(df) + 
        aes(PC1, PC2, label=samples, col=condition) + 
        geom_label(show.legend = FALSE)
p
```

```{r}
p + labs(title="PCA of RNASeq Data",
       subtitle = "PC1 clealy seperates wild-type from knock-out samples",
       x=paste0("PC1 (", pca.var.per[1], "%)"),
       y=paste0("PC2 (", pca.var.per[2], "%)"),
       caption="BIMM143 example data") +
     theme_bw()
```




